num_gpus: 16
max_num_seqs: 1024
overload_threshold: 3
gpu_memory_utilization: 0.5

models:
  -
    name: llm-0
    model: /mnt/afs/share/LLMCKPTs/huggyllama/llama-65b
    tensor_parallel_size: 16
    pipeline_parallel_size: 1
    placement:
      - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
    mps_percentage: [100]
    model_dtype: fp16
workloads:
  # workload generation refer to README
  workload_file:
